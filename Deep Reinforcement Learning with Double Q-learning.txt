Deep Reinforcement Learning with Double Q-Learning
Hado van Hasselt , Arthur Guez, and David Silver
Google DeepMind
Abstract
The popular Q-learning algorithm is known to overestimate
action values under certain conditions. It was not previouslyknown whether, in practice, such overestimations are com-
mon, whether they harm performance, and whether they can
generally be prevented. In this paper, we answer all thesequestions afﬁrmatively. In particular, we ﬁrst show that the
recent DQN algorithm, which combines Q-learning with a
deep neural network, suffers from substantial overestimationsin some games in the Atari 2600 domain. We then show that
the idea behind the Double Q-learning algorithm, which was
introduced in a tabular setting, can be generalized to work
with large-scale function approximation. We propose a spe-
ciﬁc adaptation to the DQN algorithm and show that the re-sulting algorithm not only reduces the observed overestima-
tions, as hypothesized, but that this also leads to much better
performance on several games.
The goal of reinforcement learning (Sutton and Barto 1998)
is to learn good policies for sequential decision problems,by optimizing a cumulative future reward signal. Q-learning
(Watkins 1989) is one of the most popular reinforcement
learning algorithms, but it is known to sometimes learn un-realistically high action values because it includes a maxi-mization step over estimated action values, which tends toprefer overestimated to underestimated values.
In previous work, overestimations have been attributed
to insufﬁciently ﬂexible function approximation (Thrun andSchwartz 1993) and noise (van Hasselt 2010, 2011). In thispaper, we unify these views and show overestimations canoccur when the action values are inaccurate, irrespective
of the source of approximation error. Of course, imprecise
value estimates are the norm during learning, which indi-cates that overestimations may be much more common thanpreviously appreciated.
It is an open question whether, if the overestimations
do occur, this negatively affects performance in practice.Overoptimistic value estimates are not necessarily a prob-lem in and of themselves. If all values would be uniformlyhigher then the relative action preferences are preserved andwe would not expect the resulting policy to be any worse.Furthermore, it is known that sometimes it is good to be op-timistic: optimism in the face of uncertainty is a well-known
Copyright c/circlecopyrt2016, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.exploration technique (Kaelbling et al. 1996). If, however,the overestimations are not uniform and not concentrated atstates about which we wish to learn more, then they mightnegatively affect the quality of the resulting policy. Thrunand Schwartz (1993) give speciﬁc examples in which thisleads to suboptimal policies, even asymptotically.
To test whether overestimations occur in practice and at
scale, we investigate the performance of the recent DQN al-gorithm (Mnih et al. 2015). DQN combines Q-learning witha ﬂexible deep neural network and was tested on a variedand large set of deterministic Atari 2600 games, reachinghuman-level performance on many games. In some ways,this setting is a best-case scenario for Q-learning, becausethe deep neural network provides ﬂexible function approx-imation with the potential for a low asymptotic approxima-tion error, and the determinism of the environments preventsthe harmful effects of noise. Perhaps surprisingly, we show
that even in this comparatively favorable setting DQN some-times substantially overestimates the values of the actions.
We show that the Double Q-learning algorithm (van Has-
selt 2010), which was ﬁrst proposed in a tabular setting, can
be generalized to arbitrary function approximation, includ-ing deep neural networks. We use this to construct a new al-gorithm called Double DQN. This algorithm not only yieldsmore accurate value estimates, but leads to much higherscores on several games. This demonstrates that the overes-timations of DQN indeed lead to poorer policies and that itis beneﬁcial to reduce them. In addition, by improving uponDQN we obtain state-of-the-art results on the Atari domain.
Background
To solve sequential decision problems we can learn esti-mates for the optimal value of each action, deﬁned as theexpected sum of future rewards when taking that action andfollowing the optimal policy thereafter. Under a given policyπ, the true value of an action ain a state sis
Q
π(s,a)≡E[R1+γR2+...|S0=s,A 0=a,π],
whereγ∈[0,1]is a discount factor that trades off the impor-
tance of immediate and later rewards. The optimal value isthenQ
∗(s,a)=m a x πQπ(s,a). An optimal policy is eas-
ily derived from the optimal values by selecting the highest-valued action in each state.Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)
2094Estimates for the optimal action values can be learned
using Q-learning (Watkins 1989), a form of temporal dif-
ference learning (Sutton 1988). Most interesting problems
are too large to learn all action values in all states sepa-rately. Instead, we can learn a parameterized value functionQ(s,a;θ
t). The standard Q-learning update for the param-
eters after taking action Atin stateStand observing the
immediate reward Rt+1and resulting state St+1is then
θt+1=θt+α(YQ
t−Q(St,At;θt))∇θtQ(St,At;θt).(1)
whereαis a scalar step size and the target YQ
tis deﬁned as
YQ
t≡Rt+1+γmax
aQ(St+1,a;θt). (2)
This update resembles stochastic gradient descent, updating
the current value Q(St,At;θt)towards a target value YQ
t.
Deep Q Networks
A deep Q network (DQN) is a multi-layered neural network
that for a given state soutputs a vector of action values
Q(s,·;θ), where θare the parameters of the network. For
ann-dimensional state space and an action space contain-
ingmactions, the neural network is a function from Rnto
Rm. Two important ingredients of the DQN algorithm as
proposed by Mnih et al. (2015) are the use of a target net-work, and the use of experience replay. The target network,
with parameters θ
−, is the same as the online network ex-
cept that its parameters are copied every τsteps from the
online network, so that then θ−
t=θt, and kept ﬁxed on all
other steps. The target used by DQN is then
YDQN
t≡Rt+1+γmax
aQ(St+1,a;θ−
t). (3)
For the experience replay (Lin 1992), observed transitions
are stored for some time and sampled uniformly from thismemory bank to update the network. Both the target networkand the experience replay dramatically improve the perfor-mance of the algorithm (Mnih et al. 2015).
Double Q-learning
The max operator in standard Q-learning and DQN, in (2)and (3), uses the same values both to select and to evaluatean action. This makes it more likely to select overestimatedvalues, resulting in overoptimistic value estimates. To pre-vent this, we can decouple the selection from the evaluation.
In Double Q-learning (van Hasselt 2010), two value func-
tions are learned by assigning experiences randomly to up-
date one of the two value functions, resulting in two sets ofweights, θandθ
/prime. For each update, one set of weights is
used to determine the greedy policy and the other to deter-
mine its value. For a clear comparison, we can untangle the
selection and evaluation in Q-learning and rewrite its target
(2) as
YQ
t=Rt+1+γQ(St+1,argmax
aQ(St+1,a;θt);θt).
The Double Q-learning error can then be written as
YDoubleQ
t≡Rt+1+γQ(St+1,argmax
aQ(St+1,a;θt);θ/prime
t).
(4)Notice that the selection of the action, in the argmax,i s
still due to the online weights θt. This means that, as in Q-
learning, we are still estimating the value of the greedy pol-
icy according to the current values, as deﬁned by θt.H o w -
ever, we use the second set of weights θ/prime
tto fairly evaluate
the value of this policy. This second set of weights can beupdated symmetrically by switching the roles of θandθ
/prime.
Overoptimism due to estimation errors
Q-learning’s overestimations were ﬁrst investigated byThrun and Schwartz (1993), who showed that if the actionvalues contain random errors uniformly distributed in an in-terval [−/epsilon1,/epsilon1]then each target is overestimated up to γ/epsilon1
m−1
m+1,
wheremis the number of actions. In addition, Thrun and
Schwartz give a concrete example in which these overes-timations even asymptotically lead to sub-optimal policies,and show the overestimations manifest themselves in a smalltoy problem when using function approximation. Van Has-selt (2010) noted that noise in the environment can lead tooverestimations even when using tabular representation, andproposed Double Q-learning as a solution.
In this section we demonstrate more generally that esti-
mation errors of any kind can induce an upward bias, re-gardless of whether these errors are due to environmentalnoise, function approximation, non-stationarity, or any othersource. This is important, because in practice any methodwill incur some inaccuracies during learning, simply due tothe fact that the true values are initially unknown.
The result by Thrun and Schwartz (1993) cited above
gives an upper bound to the overestimation for a speciﬁcsetup, but it is also possible, and potentially more interest-ing, to derive a lower bound.
Theorem 1. Consider a state sin which all the true optimal
action values are equal at Q
∗(s,a)=V∗(s)for someV∗(s).
LetQtbe arbitrary value estimates that are on the whole un-
biased in the sense that/summationtext
a(Qt(s,a)−V∗(s)) = 0 , but that
are not all correct, such that1
m/summationtext
a(Qt(s,a)−V∗(s))2=C
for someC> 0, wherem≥2is the number of actions in s.
Under these conditions, maxaQt(s,a)≥V∗(s)+/radicalBig
C
m−1.
This lower bound is tight. Under the same conditions, the
lower bound on the absolute error of the Double Q-learningestimate is zero. (Proof in appendix.)
Note that we did not need to assume that estimation errors
for different actions are independent. This theorem showsthat even if the value estimates are on average correct, esti-mation errors of any source can drive the estimates up andaway from the true optimal values.
The lower bound in Theorem 1 decreases with the num-
ber of actions. This is an artifact of considering the lowerbound, which requires very speciﬁc values to be attained.More typically, the overoptimism increases with the num-ber of actions as shown in Figure 1. Q-learning’s overesti-
mations there indeed increase with the number of actions,
while Double Q-learning is unbiased. As another example,if for all actions Q
∗(s,a)=V∗(s)and the estimation errors
Qt(s,a)−V∗(s)are uniformly random in [−1,1], then the
overoptimism ism−1
m+1. (Proof in appendix.)
20952481632641282565121024
number of actions0.00.51.01.5errormaxaQ(s,a)−V∗(s)
Q/prime(s,argmaxaQ(s,a))−V∗(s)
Figure 1: The orange bars show the bias in a single Q-
learning update when the action values are Q(s,a)=
V∗(s)+/epsilon1aand the errors {/epsilon1a}m
a=1are independent standard
normal random variables. The second set of action values
Q/prime, used for the blue bars, was generated identically and in-
dependently. All bars are the average of 100 repetitions.
We now turn to function approximation and consider a
real-valued continuous state space with 10 discrete actionsin each state. For simplicity, the true optimal action valuesin this example depend only on state so that in each stateall actions have the same true value. These true values areshown in the left column of plots in Figure 2 (purple lines)and are deﬁned as either Q
∗(s,a)=s i n ( s)(top row) or
Q∗(s,a)=2 e x p ( −s2)(middle and bottom rows). The left
plots also show an approximation for a single action (greenlines) as a function of state as well as the samples the es-timate is based on (green dots). The estimate is a d-degree
polynomial that is ﬁt to the true values at sampled states,whered=6 (top and middle rows) or d=9 (bottom
row). The samples match the true function exactly: there isno noise and we assume we have ground truth for the actionvalue on these sampled states. The approximation is inex-act even on the sampled states for the top two rows becausethe function approximation is insufﬁciently ﬂexible. In thebottom row, the function is ﬂexible enough to ﬁt the greendots, but this reduces the accuracy in unsampled states. No-tice that the sampled states are spaced further apart near theleft side of the left plots, resulting in larger estimation errors.In many ways this is a typical learning setting, where at eachpoint in time we only have limited data.
The middle column of plots in Figure 2 shows estimated
action values for all 10 actions (green lines), as functionsof state, along with the maximum action value in each state(black dashed line). Although the true value function is thesame for all actions, the approximations differ because theyare based on different sets of sampled states.
1The maximum
is often higher than the ground truth shown in purple on theleft. This is conﬁrmed in the right plots, which shows the dif-ference between the black and purple curves in orange. Theorange line is almost always positive, indicating an upwardbias. The right plots also show the estimates from Double
1Each action-value function is ﬁt with a different subset of in-
teger states. States −6and6are always included to avoid extrap-
olations, and for each action two adjacent integers are missing: for
actiona1states−5and−4are not sampled, for a2states−4and
−3are not sampled, and so on. This causes the estimated values to
differ.Q-learning in blue2, which are on average much closer to
zero. This demonstrates that Double Q-learning indeed can
successfully reduce the overoptimism of Q-learning.
The different rows in Figure 2 show variations of the same
experiment. The difference between the top and middle rows
is the true value function, demonstrating that overestima-tions are not an artifact of a speciﬁc true value function.The difference between the middle and bottom rows is theﬂexibility of the function approximation. In the left-middleplot, the estimates are even incorrect for some of the sam-pled states because the function is insufﬁciently ﬂexible.The function in the bottom-left plot is more ﬂexible but thiscauses higher estimation errors for unseen states, resultingin higher overestimations. This is important because ﬂexi-ble parametric function approximation is often employed inreinforcement learning (see, e.g., Tesauro 1995, Sallans andHinton 2004, Riedmiller 2005, and Mnih et al. 2015).
In contrast to van Hasselt (2010), we did not use a sta-
tistical argument to ﬁnd overestimations, the process to ob-tain Figure 2 is fully deterministic. In contrast to Thrun andSchwartz (1993), we did not rely on inﬂexible function ap-proximation with irreducible asymptotic errors; the bottomrow shows that a function that is ﬂexible enough to cover allsamples leads to high overestimations. This indicates thatthe overestimations can occur quite generally.
In the examples above, overestimations occur even when
assuming we have samples of the true action value at cer-
tain states. The value estimates can further deteriorate if webootstrap off of action values that are already overoptimistic,since this causes overestimations to propagate throughoutour estimates. Although uniformly overestimating values
might not hurt the resulting policy, in practice overestima-
tion errors will differ for different states and actions. Over-estimation combined with bootstrapping then has the perni-cious effect of propagating the wrong relative informationabout which states are more valuable than others, directly
affecting the quality of the learned policies.
The overestimations should not be confused with opti-
mism in the face of uncertainty (Sutton 1990, Agrawal 1995,Kaelbling et al. 1996, Auer at al. 2002, Brafman and Ten-nenholtz 2003, Szita and L ˜orincz 2008, Strehl and Littman
2009), where an exploration bonus is given to states or ac-tions with uncertain values. The overestimations discussedhere occur only after updating, resulting in overoptimism in
the face of apparent certainty. Thrun and Schwartz (1993)
noted that, in contrast to optimism in the face of uncertainty,these overestimations actually can impede learning an opti-mal policy. We conﬁrm this negative effect on policy qualityin our experiments: when we reduce the overestimations us-ing Double Q-learning, the policies improve.
Double DQN
The idea of Double Q-learning is to reduce overestimationsby decomposing the max operation in the target into action
2We arbitrarily used the samples of action ai+5(fori≤5)
orai−5(fori> 5) as the second set of samples for the double
estimator of action ai.
2096−202
Qt(s,a)Q∗(s,a)True value and an estimate
−202maxaQt(s,a)All estimates and max
−101maxaQt(s,a)−maxaQ∗(s,a)
Double-Q estimate+0.61
−0.02Average error Bias as function of state
02
Qt(s,a)Q∗(s,a)
02maxaQt(s,a)
−101maxaQt(s,a)−maxaQ∗(s,a)
Double-Q estimate+0.47
+0.02
−6−4−2 0 2 4 6
state024Qt(s,a)
Q∗(s,a)
−6−4−2 0 2 4 6
state024
maxaQt(s,a)
−6−4−2 0 2 4 6
state024 maxaQt(s,a)−
maxaQ∗(s,a)
Double-Q estimate+3.35
−0.02
Figure 2: Illustration of overestimations during learning. In each state (x-axis), there are 10 actions. The left column shows the
true values V∗(s)(purple line). All true action values are deﬁned by Q∗(s,a)=V∗(s). The green line shows estimated values
Q(s,a)for one action as a function of state, ﬁtted to the true value at several sampled states (green dots). The middle column
plots show all the estimated values (green), and the maximum of these values (dashed black). The maximum is higher than the
true value (purple, left plot) almost everywhere. The right column plots shows the difference in orange. The blue line in the
right plots is the estimate used by Double Q-learning with a second set of samples for each state. The blue line is much closer tozero, indicating less bias. The three rows correspond to different true functions (left, purple) or capacities of the ﬁtted function
(left, green). (Details in the text)
selection and action evaluation. Although not fully decou-
pled, the target network in the DQN architecture providesa natural candidate for the second value function, withouthaving to introduce additional networks. We therefore pro-pose to evaluate the greedy policy according to the onlinenetwork, but using the target network to estimate its value.In reference to both Double Q-learning and DQN, we referto the resulting algorithm as Double DQN. Its update is the
same as for DQN, but replacing the target Y
DQN
t with
YDoubleDQN
t ≡Rt+1+γQ(St+1,argmax
aQ(St+1,a;θt),θ−
t).
In comparison to Double Q-learning (4), the weights of the
second network θ/prime
tare replaced with the weights of the tar-
get network θ−
tfor the evaluation of the current greedy pol-
icy. The update to the target network stays unchanged fromDQN, and remains a periodic copy of the online network.
This version of Double DQN is perhaps the minimal pos-
sible change to DQN towards Double Q-learning. The goalis to get most of the beneﬁt of Double Q-learning, whilekeeping the rest of the DQN algorithm intact for a fair com-parison, and with minimal computational overhead.
Empirical results
In this section, we analyze the overestimations of DQN andshow that Double DQN improves over DQN both in terms ofvalue accuracy and in terms of policy quality. To further testthe robustness of the approach we additionally evaluate thealgorithms with random starts generated from expert humantrajectories, as proposed by Nair et al. (2015).
Our testbed consists of Atari 2600 games, using the Ar-
cade Learning Environment (Bellemare et al. 2013). Thegoal is for a single algorithm, with a ﬁxed set of hyperpa-rameters, to learn to play each of the games separately frominteraction given only the screen pixels as input. This is a de-manding testbed: not only are the inputs high-dimensional,the game visuals and game mechanics vary substantially be-tween games. Good solutions must therefore rely heavilyon the learning algorithm — it is not practically feasible tooverﬁt the domain by relying only on tuning.
We closely follow the experimental setup and network ar-
chitecture used by Mnih et al. (2015). Brieﬂy, the network
architecture is a convolutional neural network (Fukushima1988, Lecun et al. 1998) with 3 convolution layers and afully-connected hidden layer (approximately 1.5M parame-
ters in total). The network takes the last four frames as inputand outputs the action value of each action. On each game,the network is trained on a single GPU for 200M frames.
Results on overoptimism
Figure 3 shows examples of DQN’s overestimations in six
Atari games. DQN and Double DQN were both trained un-
der the exact conditions described by Mnih et al. (2015).DQN is consistently and sometimes vastly overoptimisticabout the value of the current greedy policy, as can be seenby comparing the orange learning curves in the top row ofplots to the straight orange lines, which represent the ac-tual discounted value of the best learned policy. More pre-cisely, the (averaged) value estimates are computed regu-larly during training with full evaluation phases of lengthT= 125, 000steps as
1
TT/summationdisplay
t=1argmax
aQ(St,a;θ).
2097050100 150 200101520Value estimatesAlien
050100 150 200468Space Invaders
050100 150 2001.01.52.02.5Time Pilot
050100 150 200
Training steps (in millions)02468 DQN estimate
Double DQN estimate
DQN true valueDouble DQN true valueZaxxon
0 50 100 150 200110100Value estimates
(log scale)DQN
Double DQNWizard of Wor
0 50 100 150 200510204080
DQN
Double DQNAsterix
0 50 100 150 200
Training steps (in millions)01000200030004000Score
DQNDouble DQNWizard of Wor
0 50 100 150 200
Training steps (in millions)0200040006000
DQNDouble DQNAsterix
Figure 3: The topand middle rows show value estimates by DQN (orange) and Double DQN (blue) on six Atari games. The
results are obtained by running DQN and Double DQN with 6 different random seeds with the hyper-parameters employed by
Mnih et al. (2015). The darker line shows the median over seeds and we average the two extreme values to obtain the shadedarea (i.e., 10% and 90% quantiles with linear interpolation). The straight horizontal orange (for DQN) and blue (for DoubleDQN) lines in the top row are computed by running the corresponding agents after learning concluded, and averaging the actualdiscounted return obtained from each visited state. These straight lines would match the learning curves at the right side of theplots if there is no bias. The middle row shows the value estimates (in log scale) for two games in which DQN’s overoptimism
is quite extreme. The bottom row shows the detrimental effect of this on the score achieved by the agent as it is evaluated
during training: the scores drop when the overestimations begin. Learning with Double DQN is much more stable.
The ground truth averaged values are obtained by running
the best learned policies for several episodes and computingthe actual cumulative rewards. Without overestimations wewould expect these quantities to match up (i.e., the curve tomatch the straight line at the right of each plot). Instead, thelearning curves of DQN consistently end up much higherthan the true values. The learning curves for Double DQN,shown in blue, are much closer to the blue straight line rep-resenting the true value of the ﬁnal policy. Note that the bluestraight line is often higher than the orange straight line. Thisindicates that Double DQN does not just produce more ac-curate value estimates but also better policies.
More extreme overestimations are shown in the middle
two plots, where DQN is highly unstable on the games As-terix and Wizard of Wor. Notice the log scale for the valueson they-axis. The bottom two plots shows the correspond-
ing scores for these two games. Notice that the increases invalue estimates for DQN in the middle plots coincide withdecreasing scores in bottom plots. Again, this indicates thatthe overestimations are harming the quality of the resultingpolicies. If seen in isolation, one might perhaps be temptedto think the observed instability is related to inherent in-stability problems of off-policy learning with function ap-proximation (Baird 1995, Tsitsiklis and Van Roy 1997, Maeino ops human starts
DQN DDQN DQN DDQN DDQN
(tuned)
Median 93% 115% 47% 88% 117%
Mean 241% 330% 122% 273% 475%
Table 1: Summarized normalized performance on 49 games
for up to 5 minutes with up to 30 no ops at the start of eachepisode, and for up to 30 minutes with randomly selectedhuman start points. Results for DQN are from Mnih et al.(2015) (no ops) and Nair et al. (2015) (human starts).
2011, Sutton et al. 2015). However, we see that learning is
much more stable with Double DQN, suggesting that the
cause for these instabilities is in fact Q-learning’s overopti-
mism. Figure 3 only shows a few examples, but overestima-tions were observed for DQN in all 49 tested Atari games,albeit in varying amounts.
Quality of the learned policies
Overoptimism does not always adversely affect the qualityof the learned policy. For example, DQN achieves optimal
2098behavior in Pong despite slightly overestimating the policy
value. Nevertheless, reducing overestimations can signiﬁ-
cantly beneﬁt the stability of learning; we see clear examples
of this in Figure 3. We now assess more generally how muchDouble DQN helps in terms of policy quality by evaluatingon all 49 games that DQN was tested on.
As described by Mnih et al. (2015) each evaluation
episode starts by executing a special no-op action that doesnot affect the environment up to 30 times, to provide differ-ent starting points for the agent. Some exploration duringevaluation provides additional randomization. For DoubleDQN we used the exact same hyper-parameters as for DQN,
to allow for a controlled experiment focused just on re-
ducing overestimations. The learned policies are evaluatedfor 5 mins of emulator time (18,000 frames) with an /epsilon1-
greedy policy where /epsilon1=0.05. The scores are averaged over
100 episodes. The only difference between Double DQN
and DQN is the target, using Y
DoubleDQN
t rather than YDQN.
This evaluation is somewhat adversarial, as the used hyper-
parameters were tuned for DQN but not for Double DQN.
To obtain summary statistics across games, we normalize
the score for each game as follows:
score normalized =score agent−score random
score human−score random. (5)
The ‘random’ and ‘human’ scores are the same as used by
Mnih et al. (2015), and are given in the appendix.
Table 1, under no ops, shows that on the whole Double
DQN clearly improves over DQN. A detailed comparison(in appendix) shows that there are several games in whichDouble DQN greatly improves upon DQN. Noteworthy ex-amples include Road Runner (from 233% to 617%), Asterix(from 70% to 180%), Zaxxon (from 54% to 111%), andDouble Dunk (from 17% to 397%).
The Gorila algorithm (Nair et al. 2015), which is a mas-
sively distributed version of DQN, is not included in the ta-ble because the architecture and infrastructure is sufﬁcientlydifferent to make a direct comparison unclear. For complete-ness, we note that Gorila obtained median and mean normal-ized scores of 96% and 495%, respectively.
Robustness to Human starts
One concern with the previous evaluation is that in deter-
ministic games with a unique starting point the learner could
potentially learn to remember sequences of actions with-out much need to generalize. While successful, the solutionwould not be particularly robust. By testing the agents fromvarious starting points, we can test whether the found so-lutions generalize well, and as such provide a challengingtestbed for the learned polices (Nair et al. 2015).
We obtained 100 starting points sampled for each game
from a human expert’s trajectory, as proposed by Nair et al.(2015). We start an evaluation episode from each of thesestarting points and run the emulator for up to 108,000 frames(30 mins at 60Hz including the trajectory before the startingpoint). Each agent is only evaluated on the rewards accumu-lated after the starting point.
For this evaluation we include a tuned version of Double
DQN. Some tuning is appropriate because the hyperparame-0% 100% 200% 300% 400% 500%1000%1500%2000%2500%5000%7500%
Normalized score
Human
∗∗Solaris∗∗Private EyeGravitarVentureMontezuma’s RevengeAsteroids∗∗Pitfall∗∗Ms. PacmanAmidar∗∗Yars Revenge ∗∗AlienCentipedeBowling∗∗Skiing∗∗FrostbiteChopper CommandSeaquest∗∗Berzerk ∗∗H.E.R.O.TutankhamIce HockeyBattle ZoneRiver Raid∗∗Surround ∗∗Q*BertTennisFishing DerbyZaxxonPongFreewayBeam RiderBank HeistTime PilotName This GameWizard of WorKung-Fu MasterEnduroJames BondSpace InvadersUp and Down∗∗Phoenix ∗∗∗∗Defender ∗∗AsterixKangarooCrazy ClimberKrullRoad RunnerStar GunnerBoxingGopherRobotankDouble DunkAssaultBreakoutDemon AttackAtlantisVideo Pinball
Double DQN (tuned)
Double DQN
DQN
Figure 4: Normalized scores on 57 Atari games, testedfor 100 episodes per game with human starts. Comparedto Mnih et al. (2015), eight games additional games weretested. These are indicated with stars and a bold font.
ters were tuned for DQN, which is a different algorithm. For
the tuned version of Double DQN, we increased the num-ber of frames between each two copies of the target networkfrom 10,000 to 30,000, to reduce overestimations further be-cause immediately after each switch DQN and Double DQNboth revert to Q-learning. In addition, we reduced the explo-ration during learning from /epsilon1=0.1to/epsilon1=0.01, and then
used/epsilon1=0.001during evaluation. Finally, the tuned ver-
sion uses a single shared bias for all action values in the toplayer of the network. Each of these changes improved per-formance and together they result in clearly better results.
3
Table 1 reports summary statistics for this evaluation
(under human starts) on the 49 games from Mnih et al.
(2015). Double DQN obtains clearly higher median and
3Except for Tennis, where the lower /epsilon1during training seemed
to hurt rather than help.
2099mean scores. Again Gorila DQN (Nair et al. 2015) is not
included in the table, but for completeness note it obtained a
median of 78% and a mean of 259%. Detailed results, plus
results for an additional 8 games, are available in Figure 4and in the appendix. On several games the improvementsfrom DQN to Double DQN are striking, in some cases bring-ing scores much closer to human, or even surpassing these.
Double DQN appears more robust to this more challeng-
ing evaluation, suggesting that appropriate generalizationsoccur and that the found solutions do not exploit the deter-minism of the environments. This is appealing, as it indi-cates progress towards ﬁnding general solutions rather thana deterministic sequence of steps that would be less robust.
Discussion
This paper has ﬁve contributions. First, we have shown whyQ-learning can be overoptimistic in large-scale problems,even if these are deterministic, due to the inherent estima-tion errors of learning. Second, by analyzing the value es-timates on Atari games we have shown that these overesti-mations are more common and severe in practice than pre-viously acknowledged. Third, we have shown that DoubleQ-learning can be used at scale to successfully reduce thisoveroptimism, resulting in more stable and reliable learning.Fourth, we have proposed a speciﬁc implementation calledDouble DQN, that uses the existing architecture and deepneural network of the DQN algorithm without requiring ad-ditional networks or parameters. Finally, we have shown thatDouble DQN ﬁnds better policies, obtaining new state-of-the-art results on the Atari 2600 domain.
Acknowledgments
We would like to thank Tom Schaul, V olodymyr Mnih, Marc
Bellemare, Thomas Degris, Georg Ostrovski, and Richard
Sutton for helpful comments, and everyone at Google Deep-Mind for a constructive research environment.
References
R. Agrawal. Sample mean based index policies with O(log n) regret
for the multi-armed bandit problem. Advances in Applied Proba-
bility, pages 1054–1078, 1995.
P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the
multiarmed bandit problem. Machine learning, 47(2-3):235–256,
2002.
L. Baird. Residual algorithms: Reinforcement learning with func-
tion approximation. In Machine Learning: Proceedings of the
Twelfth International Conference, pages 30–37, 1995.
M. G. Bellemare, Y . Naddaf, J. Veness, and M. Bowling. The
arcade learning environment: An evaluation platform for generalagents. J. Artif. Intell. Res. (JAIR), 47:253–279, 2013.
R. I. Brafman and M. Tennenholtz. R-max-a general polynomialtime algorithm for near-optimal reinforcement learning. The Jour-
nal of Machine Learning Research, 3:213–231, 2003.
K. Fukushima. Neocognitron: A hierarchical neural network capa-
ble of visual pattern recognition. Neural networks, 1(2):119–130,
1988.
L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement
learning: A survey. Journal of Artiﬁcial Intelligence Research,4 :
237–285, 1996.Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-basedlearning applied to document recognition. Proceedings of the
IEEE, 86(11):2278–2324, 1998.
L. Lin. Self-improving reactive agents based on reinforcement
learning, planning and teaching. Machine learning, 8(3):293–321,
1992.
H. R. Maei. Gradient temporal-difference learning algorithms.
PhD thesis, University of Alberta, 2011.V . Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostro-vski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-levelcontrol through deep reinforcement learning. Nature, 518(7540):
529–533, 2015.
A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. D.
Maria, V . Panneershelvam, M. Suleyman, C. Beattie, S. Petersen,
S. Legg, V . Mnih, K. Kavukcuoglu, and D. Silver. Massively par-
allel methods for deep reinforcement learning. In Deep Learning
Workshop, ICML, 2015.
M. Riedmiller. Neural ﬁtted Q iteration - ﬁrst experiences with a
data efﬁcient neural reinforcement learning method. In Proceed-
ings of the 16th European Conference on Machine Learning , pages
317–328. Springer, 2005.
B. Sallans and G. E. Hinton. Reinforcement learning with factored
states and actions. The Journal of Machine Learning Research,5 :
1063–1088, 2004.
A. L. Strehl, L. Li, and M. L. Littman. Reinforcement learning
in ﬁnite MDPs: PAC analysis. The Journal of Machine Learning
Research, 10:2413–2444, 2009.
R. S. Sutton. Learning to predict by the methods of temporal dif-
ferences. Machine learning, 3(1):9–44, 1988.
R. S. Sutton. Integrated architectures for learning, planning, and
reacting based on approximating dynamic programming. In Pro-
ceedings of the seventh international conference on machine learn-
ing, pages 216–224, 1990.
R. S. Sutton and A. G. Barto. Introduction to reinforcement learn-
ing. MIT Press, 1998.
R. S. Sutton, A. R. Mahmood, and M. White. An emphatic ap-
proach to the problem of off-policy temporal-difference learning.
arXiv preprint arXiv:1503.04269, 2015.
I. Szita and A. L ˝orincz. The many faces of optimism: a unifying
approach. In Proceedings of the 25th international conference on
Machine learning, pages 1048–1055. ACM, 2008.G. Tesauro. Temporal difference learning and td-gammon. Com-
munications
of the ACM, 38(3):58–68, 1995.
S. Thrun and A. Schwartz. Issues in using function approxi-
mation for reinforcement learning. In M. Mozer, P. Smolensky,D. Touretzky, J. Elman, and A. Weigend, editors, Proceedings
of the 1993 Connectionist Models Summer School, Hillsdale, NJ,
1993. Lawrence Erlbaum.
J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference
learning with function approximation. IEEE Transactions on Au-
tomatic Control, 42(5):674–690, 1997.
H. van Hasselt. Double Q-learning. Advances in Neural Informa-
tion Processing Systems, 23:2613–2621, 2010.H. van Hasselt. Insights in Reinforcement Learning. PhD thesis,
Utrecht University, 2011.
C. J. C. H. Watkins. Learning from delayed rewards . PhD thesis,
University of Cambridge England, 1989.
2100